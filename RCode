#Load an R data frame.
#first of all set working directory

#install packages and load libraries

#install.packages("plyr") #Tools for Splitting, Applying and Combining Data
#install.packages("dplyr") #dplyr is a grammar of data manipulation,
#install.packages('ggplot2')
#install.packages('caret') #creating predictive model
#install.packages('rattle')
library(rattle)  #providing a graphical user interface
library(magrittr) # For the %>% and %<>% operators.
library(caret)
library(lattice)
library(ggplot2)
library(dplyr)
library(plyr)

#Load CSV file
Employee_Turnover <- read.csv("~/EmpTurnover.csv")
MYdataset <- Employee_Turnover

#data quality
str(MYdataset)
summary(MYdataset)
head(MYdataset)
sum(is.na(MYdataset)) #missing values

#initial look
StatusCount<- as.data.frame.matrix(MYdataset %>%
                                     group_by(STATUS_YEAR) %>%
                                     select(STATUS) %>%
                                     table())
StatusCount$TOTAL<-StatusCount$ACTIVE + StatusCount$TERMINATED
StatusCount$PercentTerminated <-StatusCount$TERMINATED/(StatusCount$TOTAL)*100
StatusCount
mean(StatusCount$PercentTerminated)

#plotting Business Unit to dertermine the ratio of Active to Terminated by status
ggplot() + geom_bar(aes(y = ..count..,x =as.factor(BUSINESS_UNIT),
                        fill = as.factor(STATUS)),
                    data=MYdataset,position = position_stack())


#just terminates
TerminatesData<- as.data.frame(MYdataset %>%
                                 filter(STATUS=="TERMINATED"))

#status_year by termtype_desc
ggplot() + geom_bar(aes(y = ..count..,x =as.factor(STATUS_YEAR),
                        fill = as.factor(termtype_desc)),
                    data=TerminatesData,position = position_stack())


#status_year by termreason_desc
ggplot() + geom_bar(aes(y = ..count..,x =as.factor(STATUS_YEAR),
                        fill = as.factor(termreason_desc)),
                    data=TerminatesData, position = position_stack())


#department_name by termreason_desc
ggplot() + geom_bar(aes(y = ..count..,x =as.factor(department_name),
                        fill = as.factor(termreason_desc)),
                    data=TerminatesData,position = position_stack())+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))



#feature plot age and length of service
featurePlot(x=MYdataset[,6:7],y=MYdataset$STATUS,plot="density",
            auto.key = list(columns = 2))

#box plot age and length
featurePlot(x=MYdataset[,6:7],y=MYdataset$STATUS,plot="box",auto.key = list(columns = 2))

                    ## modelling ##

# Classification Models
# Decision Tree 
# The 'rpart' package provides the 'rpart' function.
library(rattle)
library(rpart, quietly=TRUE)

# Reset the random number seed to obtain the same results each time.
set.seed(crv$seed)

# Build the Decision Tree model.
MYrpart <- rpart(STATUS ~ .,
                 data=MYTrainingData,
                 method="class",
                 parms=list(split="information"),
                 control=rpart.control(usesurrogate=0, 
                                       maxsurrogate=0))

# Generate a textual view of the Decision Tree model.
print(MYrpart)

MYpr <- predict(MYrpart, newdata=MYtest[c(MYinput, MYtarget)], type="class")
count(data.frame(MYpr))


# Plot the resulting Decision Tree. 
# We use the rpart.plot package.

fancyRpartPlot(MYrpart, main="Decision Tree MFG10YearTerminationData $ STATUS")

#install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(MYrpart, extra = 4)




#============================================================
# Rattle timestamp: 2016-03-25 18:21:29 x86_64-w64-mingw32
# Random Forest
# The 'randomForest' package provides the 'randomForest' function.

#install.packages("randomForest")
library(randomForest, quietly=TRUE)

# Build the Random Forest model.
set.seed(crv$seed)
MYrf <- randomForest::randomForest(STATUS ~ .,
                                   data=MYtrain[c(MYinput, MYtarget)],
                                   ntree=500,
                                   mtry=2,
                                   importance=TRUE,
                                   na.action=randomForest::na.roughfix,
                                   replace=FALSE)

# Generate textual output of 'Random Forest' model.
MYrf

library(pROC)

# The `pROC' package implements various AUC functions.
# Calculate the Area Under the Curve (AUC).
pROC::roc(MYrf$y, as.numeric(MYrf$predicted))

# Calculate the AUC Confidence Interval.
pROC::ci.auc(MYrf$y, as.numeric(MYrf$predicted))

# List the importance of the variables.
rn <- round(randomForest::importance(MYrf), 2)
rn[order(rn[,3], decreasing=TRUE),]

# Time taken: 18.66 secs

#============================================================
# Rattle timestamp: 2016-03-25 18:22:22 x86_64-w64-mingw32 

# Ada Boost 

# The `ada' package implements the boost algorithm.

# Build the Ada Boost model.

set.seed(crv$seed)
install.packages("ada")
library(ada)

MYada <- ada::ada(STATUS ~ .,
                  data=MYtrain[c(MYinput, MYtarget)],
                  control=rpart::rpart.control(maxdepth=30,
                                               cp=0.010000,
                                               minsplit=20,
                                               xval=10),
                  iter=50)

# Print the results of the modelling.
print(MYada)
round(MYada$model$errs[MYada$iter,], 2)
cat('Variables actually used in tree construction:\n')

install.packages('listAdaVarsUsed')
library(listAdaVarsUsed)

print(sort(names(listAdaVarsUsed(MYada))))
cat('\nFrequency of variables actually used:\n')
print(listAdaVarsUsed(MYada))

# Time taken: 27.73 secs

#============================================================
# Rattle timestamp: 2016-03-25 18:22:56 x86_64-w64-mingw32 

# Support vector machine. 

# The 'kernlab' package provides the 'ksvm' function.
install.packages("kernlab")
library(kernlab, quietly=TRUE)

# Build a Support Vector Machine model.

set.seed(crv$seed)
MYksvm <- ksvm(as.factor(STATUS) ~ .,
               data=MYtrain[c(MYinput, MYtarget)],
               kernel="rbfdot",
               prob.model=TRUE)

# Generate a textual view of the SVM model.

MYksvm

# Time taken: 42.91 secs

#============================================================
# Rattle timestamp: 2016-03-25 18:23:56 x86_64-w64-mingw32 

# Regression model 

# Build a Regression model.

MYglm <- glm(STATUS ~ .,
             data=MYtrain[c(MYinput, MYtarget)],
             family=binomial(link="logit"))

# Generate a textual view of the Linear model.

print(summary(MYglm))
cat(sprintf("Log likelihood: %.3f (%d df)\n",
            logLik(MYglm)[1],
            attr(logLik(MYglm), "df")))
cat(sprintf("Null/Residual deviance difference: %.3f (%d df)\n",
            MYglm$null.deviance-MYglm$deviance,
            MYglm$df.null-MYglm$df.residual))
cat(sprintf("Chi-square p-value: %.8f\n",
            dchisq(MYglm$null.deviance-MYglm$deviance,
                   MYglm$df.null-MYglm$df.residual)))
cat(sprintf("Pseudo R-Square (optimistic): %.8f\n",
            cor(MYglm$y, MYglm$fitted.values)))
cat('\n==== ANOVA ====\n\n')
print(anova(MYglm, test="Chisq"))
cat("\n")

# Time taken: 1.62 secs

#============================================================
# Rattle timestamp: 2016-03-25 18:50:22 x86_64-w64-mingw32 

# Evaluate model performance. 
# Generate an Error Matrix for the Decision Tree model.
# Obtain the response from the Decision Tree model.

MYpr <- predict(MYrpart, newdata=MYtest[c(MYinput, MYtarget)], type="class")

# Generate the confusion matrix showing counts.
table(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.

pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.

cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.

cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))

# Generate an Error Matrix for the Ada Boost model.

# Obtain the response from the Ada Boost model.

MYpr <- predict(MYada, newdata=MYtest[c(MYinput, MYtarget)])

# Generate the confusion matrix showing counts.

table(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.

pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.

cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.

cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))


# Generate an Error Matrix for the Random Forest model.
# Obtain the response from the Random Forest model.
MYpr <- predict(MYrf, newdata=na.omit(MYtest[c(MYinput, MYtarget)]))

# Generate the confusion matrix showing counts.
table(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.
pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.
cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.
cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))


# Generate an Error Matrix for the SVM model.
# Obtain the response from the SVM model.
MYpr <- kernlab::predict(MYksvm, newdata=na.omit(MYtest[c(MYinput, MYtarget)]))

# Generate the confusion matrix showing counts.
table(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.
pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.
cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.
cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))


# Generate an Error Matrix for the Linear model.
# Obtain the response from the Linear model.
MYpr <- as.vector(ifelse(predict(MYglm, type="response", newdata=MYtest[c(MYinput, MYtarget)]) > 0.5, "TERMINATED", "ACTIVE"))

# Generate the confusion matrix showing counts.
table(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.
pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.
cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.
cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))
